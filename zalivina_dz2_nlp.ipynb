{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    \n",
    "    NewsMorphTagger,\n",
    "    \n",
    "    Doc\n",
    ")\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.util import ngrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_0.txt', 'r', encoding='utf-8') as t:\n",
    "    text = t.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace('\\n', ' ')\n",
    "text = text.replace('\\u205f', ' ')\n",
    "text_result = re.sub(r'[A-Za-z]', '', text)\n",
    "text_ = re.sub(r'[\"«»]', '', text_result)\n",
    "text_full = re.sub(r\"\\s[-—](\\s)?\", ' ', text_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный текст подходит, так как есть словоформы с неизвестным корнем, но образованные с помощью продуктивных аффиксов (спотов, чиллит), есть словоформы с дефисом, в некоторых случаях его нельзя отделять (ей-богу, туда-сюда, перекати-поле), слова с неизвестными корнями, не содержащие продуктивных аффиксов (трэп, расслабон, грев), нестандартные формы (скрежет - может определится как глагол по окончанию или как существительное)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'И как я очутился, сдуру, где вечный сюр посередине жизни в сумеречном лесу? Я несу и так едва крест, я те не Иса. Где дорога? Ей-богу, дай упорюсь. Дай кредитку, не горячись и не кипишуй. Кто торчит? Меня, жить не учи, мы по чуть-чуть.   шик или китч, всё чересчур, судя по их глазам, все повеселели весьма. Челюсть туда-сюда, заледенела десна. Честно, я сам не знаю, ребзя, кем же я стал. Я всего лишь писатель, моё дело писать. Будапешт пропитан красным Данхиллом из . Ну как там вы, любимые мосты? Как там ты, брат-родник? Поднимаю голову вверх, чтобы увидеть на миг твой лик и солнца блик. Подмигнул на удачу, ты со мной навечно, брачо. Вокруг много спотов, но дом может быть лишь один. Мой дом где живут мои люди, но мои люди обжили весь мир. Кто-то живет в Торонто, кто-то чиллит в . Для кого-то дом это Бостон, мой отряд из Уфы. В моем загране так много штампов. Я не был в Риме, я не был в Тампе. Я был на небе, я был в капкане. Мне нужно место между двух палем. Перекати-поле, я как перекати-поле. Я кручусь как роллы, роллы Калифорния. Я хотел бы знать, откуда ветер дует в мои кудри, но я снова обгоняю, обгоняю эти будни. Если мы видим мусоров, то слышишь скрежет шин. Дыма столько ароматного, как от кадила. Мой слайм не оставит меня в беде, твой кент опрокинет за мятый чек. Я просто хочу огромный стейк с бейсбольную биту, горит штакет. Роллю под луной, чтоб убить боль, платиновый свет, это швэпс-понт. У Лимбы за спиной катана острая, как чили. Я раз слабился, потом два слабился, потом три слабился. Рядом мои бро, мы делим расслабон. Да, я на чиле, на расслабоне, я не спешу, времени море. Копы много говорят, но разговоры это блеф. Буду делать много дел, если брату нужен грев. На иконе тихо бдит святой. Разогнался, прыгнул в трэп с головой. Закупаю эти гуны, все на кругленькую сумму. Тебе никто не поверит, базу сносим за минуту. С Депо вышибаем двери, лёжа, понял, кто мы и откуда.'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тегсет был выбран на основе udpipe, как наиболеее универсальный для остальных трех парсеров (именно для этого тегсета не потребуется много исправлений). DET объединен с PRON, AUX c VERB и все союзы объединены в CONJ (так как некоторые парсеры не различают сочинительные и подчинительные союзы). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('words.csv', 'r', encoding='utf-8') as t:\n",
    "    words = t.read()\n",
    "words = words.split('\\n')\n",
    "wordlist = []\n",
    "for word in words:\n",
    "    word_split = word.split(',')\n",
    "    wordlist.append([word_split[0], word_split[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()\n",
    "ana = m.analyze(text_full)\n",
    "word_pos_mystem = []\n",
    "for word in ana:\n",
    "    if 'analysis' in word:\n",
    "        gr = word['analysis'][0]['gr']\n",
    "        pos = gr.split('=')[0].split(',')[0]\n",
    "        if pos == 'V':\n",
    "            pos = 'VERB'\n",
    "        if pos == 'S':\n",
    "            pos = 'NOUN'\n",
    "        if pos == 'A':\n",
    "            pos = 'ADJ'\n",
    "        if pos == 'SPRO' or pos == 'APRO':\n",
    "            pos = 'PRON'\n",
    "        if pos == 'PR':\n",
    "            pos = 'ADP'\n",
    "        if pos == 'ADVPRO':\n",
    "            pos = 'CONJ'\n",
    "        word_pos_mystem.append([word['text'], pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pos_mystem_clear = [] \n",
    "for word_pos in word_pos_mystem:\n",
    "    if not (word_pos == ['понт', 'NOUN'] or word_pos == ['родник', 'NOUN']):\n",
    "        word_pos_mystem_clear.append(word_pos)\n",
    "# Mystem разделил две словоформы существительных, содержащих дефис, \n",
    "# поэтому было принято решение удалить появившиеся лишние токены, чтобы посчитать accuracy.\n",
    "# Разделение на токены не повлияло на POS tagging, так как при разделении mystem указал \n",
    "# для двух частей словоформы одну и ту же часть речи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = []\n",
    "for word_pos in wordlist:\n",
    "    gold.append(word_pos[1])\n",
    "results_mystem = []\n",
    "for word_pos in word_pos_mystem_clear:\n",
    "    results_mystem.append(word_pos[1])\n",
    "accuracy_mystem = accuracy_score(gold, results_mystem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.893491124260355"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_mystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()\n",
    "text = text_full.replace('.', '')\n",
    "text = text.replace('?', '')\n",
    "text = text.replace(',', '')\n",
    "text = text.lower()\n",
    "text_splitted = text.split()\n",
    "word_pos_pymorphy = []\n",
    "for word in text_splitted:\n",
    "    ana = morph.parse(word)\n",
    "    first = ana[0]\n",
    "    pos = first.tag.POS\n",
    "    if pos == 'ADJF' or pos == 'ADJS':\n",
    "        pos = 'ADJ'\n",
    "    if pos == 'INFN' or pos == 'GRND':\n",
    "        pos = 'VERB'\n",
    "    if pos == 'NUMR':\n",
    "        pos = 'NUM'\n",
    "    if pos == 'ADVB':\n",
    "        pos = 'ADV'\n",
    "    if pos == 'NPRO':\n",
    "        pos = 'PRON'\n",
    "    if pos == 'PRCL':\n",
    "        pos = 'PART'\n",
    "    if pos == None:\n",
    "        pos = 'NONE'\n",
    "    word_pos_pymorphy.append([word, pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pymorphy = []\n",
    "for word_pos in word_pos_pymorphy:\n",
    "    results_pymorphy.append(word_pos[1])\n",
    "accuracy_pymorphy = accuracy_score(gold, results_pymorphy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.772189349112426"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_pymorphy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = Segmenter()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "doc = Doc(text_full)\n",
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)\n",
    "word_pos_natasha = []\n",
    "for token in doc.tokens:\n",
    "    pos = token.pos\n",
    "    if pos == 'SCONJ' or pos == 'CCONJ':\n",
    "        pos = 'CONJ'\n",
    "    if pos == 'AUX':\n",
    "        pos = 'VERB'\n",
    "    if pos == 'DET':\n",
    "        pos = 'PRON'\n",
    "    if pos != 'PUNCT':\n",
    "        word_pos_natasha.append([token.text, pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_natasha = []\n",
    "for word_pos in word_pos_natasha:\n",
    "    results_natasha.append(word_pos[1])\n",
    "accuracy_natasha = accuracy_score(gold, results_natasha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8698224852071006"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_natasha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunker for mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(word_pos_mystem, pos_1, pos_2):\n",
    "    if pos_1 != 'не':\n",
    "        token_1 = ''\n",
    "        bigramm = []\n",
    "        for i in range (0, len(word_pos_mystem)):\n",
    "            if token_1 == '':\n",
    "                if word_pos_mystem[i][1] == pos_1:\n",
    "                    token_1 = word_pos_mystem[i][0]\n",
    "            else:\n",
    "                if word_pos_mystem[i][1] == pos_2 and word_pos_mystem[i][0][0].islower():\n",
    "                    bigramm.append([token_1, word_pos_mystem[i][0]])\n",
    "                    token_1 = ''\n",
    "                elif word_pos_mystem[i][1] != pos_1:\n",
    "                    token_1 = ''  \n",
    "                else:\n",
    "                    token_1 = word_pos_mystem[i][0]\n",
    "                \n",
    "    if pos_1 == 'не':\n",
    "        bigramm = []\n",
    "        for i in range (0, len(word_pos_mystem)):\n",
    "            if word_pos_mystem[i][0] == part and (word_pos_mystem[i+1][1] == 'VERB' or word_pos_mystem[i+1][1] == 'NOUN'):\n",
    "                bigramm.append([part, word_pos_mystem[i+1][0]])\n",
    "            if word_pos_mystem[i][0] == part and word_pos_mystem[i+1][1] == 'ADJ' and word_pos_mystem[i+2][1] == 'NOUN':\n",
    "                bigramm.append([part, word_pos_mystem[i+2][0]])\n",
    "    \n",
    "    return bigramm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_noun = chunker(word_pos_mystem, 'ADJ', 'NOUN')\n",
    "neg = chunker(word_pos_mystem, 'не', None)\n",
    "v_a = chunker(word_pos_mystem, 'VERB', 'ADV')\n",
    "a_v = chunker(word_pos_mystem, 'ADV', 'VERB')\n",
    "adv_verb = v_a + a_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Первое дз"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=UTF-8\n",
    "from bs4 import BeautifulSoup \n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writing_in_dict(soup):\n",
    "    d_com = {}\n",
    "    text_full = soup.find_all(\"div\", attrs={\"class\": \"responses__item__message markup-inside-small markup-inside-small--bullet\"})\n",
    "    regex = r\"<.+>\"\n",
    "    comments_clean = []\n",
    "    for each in text_full:\n",
    "        result = re.sub(regex, '', str(each), 0, re.MULTILINE)\n",
    "        result = result.replace('\\n', '')\n",
    "        result = result.replace('\\t', '')\n",
    "        result = result.replace('\\xa0', '')\n",
    "        comments_clean.append(result)\n",
    "        time.sleep(random.uniform(3.2,5.4))\n",
    "    return comments_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [05:21<00:00, 107.25s/it]\n",
      "100%|██████████| 3/3 [05:23<00:00, 107.77s/it]\n"
     ]
    }
   ],
   "source": [
    "# x - оценка, y - страница \n",
    "database = {}\n",
    "votes = [1,5]\n",
    "for x in votes:\n",
    "    for y in tqdm(range(1,4)):\n",
    "        url = 'https://www.banki.ru/services/responses/bank/sberbank/?rate='+ str(x) +'&page='+ str(y) + '&isMobile=0'\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        d = writing_in_dict(soup)\n",
    "        a = str(x)+str(y)\n",
    "        database[a] = d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "database['11'].extend(database['12'])\n",
    "database['51'].extend(database['52'])\n",
    "database['13'].extend(database['53'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_dict = {}\n",
    "pos_list = []\n",
    "neg_list = []\n",
    "for each in database['13']:\n",
    "    if each != '':\n",
    "        if each in database['53']:\n",
    "            pos_list.append(each)\n",
    "        else:\n",
    "            neg_list.append(each)\n",
    "control_dict['pos'] = pos_list\n",
    "control_dict['neg'] = neg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_coments = database['11']\n",
    "pos_coments = database['51']\n",
    "control_group = database['13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_comments_30 = random.sample(neg_coments, 30)\n",
    "pos_comments_30 = random.sample(pos_coments, 30)\n",
    "control = random.sample(control_group, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_pos = ''.join(pos_comments_30)\n",
    "str_neg = ''.join(neg_comments_30)\n",
    "str_control = ''.join(control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_pos = [w.lower() for w in word_tokenize(str_pos) if w.isalpha()]\n",
    "words_neg = [w.lower() for w in word_tokenize(str_neg) if w.isalpha()]\n",
    "sw = stopwords.words('russian')\n",
    "filtered_pos = [w for w in words_pos if w not in sw]\n",
    "filtered_neg = [w for w in words_neg if w not in sw]\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "lemmas_pos = []\n",
    "for word in filtered_pos:\n",
    "    lemmas_pos.append(morph.parse(word)[0].normal_form)\n",
    "lemmas_neg = []\n",
    "for word in filtered_neg:\n",
    "    lemmas_neg.append(morph.parse(word)[0].normal_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_pos = Counter()\n",
    "for word in lemmas_pos:\n",
    "    cnt_pos[word] += 1\n",
    "cnt_neg = Counter()\n",
    "for word in lemmas_neg:\n",
    "    cnt_neg[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_pos = {}\n",
    "d_neg = {}\n",
    "for key in dict(cnt_pos):\n",
    "    if key not in dict(cnt_neg).keys() and cnt_pos[key]>1:\n",
    "        d_pos[key] = cnt_pos[key]\n",
    "for key in dict(cnt_neg):\n",
    "    if key not in dict(cnt_pos).keys() and cnt_neg[key]>1:\n",
    "        d_neg[key] = cnt_neg[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Были выделены следующие шаблоны: выразить + NOUN (на основе множественных примеров \"выразить благодарность\", \"выразить восхищение\") для положительного словаря;  VERB + что для негативного словаря, так как негативные отзывы характеризуются длинными описаниями ситуации (сказала, что; оказалось, что и т.д.). Универсальными стали шаблоны VERB + NOUN и ADJ + NOUN (соответственно, глаголы с прямыми дополнениями и прилагательные с существительными). То есть, получается действие + на что направлено действие (в основном) и описание + существительное. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker_neg(word_pos_mystem, pos_1, pos_2):\n",
    "    if pos_1 == 'VERB' and pos_2 == 'что':\n",
    "        bigramm = []\n",
    "        for sent in word_pos_mystem:\n",
    "            for i in range (0, len(sent)):\n",
    "                try:\n",
    "                    if sent[i][1] == pos_1 and sent[i+1][0] == pos_2:\n",
    "                        bigramm.append([sent[i][0], sent[i+1][0]])\n",
    "                except:\n",
    "                    pass\n",
    "    if (pos_1 == 'VERB' and pos_2 == 'NOUN') or (pos_1 == 'ADJ' and pos_2 == 'NOUN'):\n",
    "        bigramm = []\n",
    "        for sent in word_pos_mystem:\n",
    "            for i in range (0, len(sent)):\n",
    "                try:\n",
    "                    if sent[i][1] == pos_1 and sent[i+1][1] == pos_2:\n",
    "                        bigramm.append([sent[i][0], sent[i+1][0]])\n",
    "                except:\n",
    "                    pass\n",
    "    return bigramm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker_pos(word_pos_mystem, pos_1, pos_2):\n",
    "    if pos_1 == 'выразить' and pos_2 == 'NOUN':\n",
    "        bigramm = []\n",
    "        for sent in word_pos_mystem:\n",
    "            for i in range (0, len(sent)):\n",
    "                try:\n",
    "                    if sent[i][0] == pos_1 and sent[i+1][1] == 'ADJ' and sent[i+2][1] == pos_2:\n",
    "                        bigramm.append([sent[i][0], sent[i+2][0]])\n",
    "                except:\n",
    "                    pass\n",
    "    if (pos_1 == 'VERB' and pos_2 == 'NOUN') or (pos_1 == 'ADJ' and pos_2 == 'NOUN'):\n",
    "        bigramm = []\n",
    "        for sent in word_pos_mystem:\n",
    "            for i in range (0, len(sent)):\n",
    "                try:\n",
    "                    if sent[i][1] == pos_1 and sent[i+1][1] == pos_2:\n",
    "                        bigramm.append([sent[i][0], sent[i+1][0]])\n",
    "                except:\n",
    "                    pass\n",
    "    return bigramm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = []\n",
    "for comment in pos_coments:\n",
    "    ana = m.analyze(comment)\n",
    "    word_pos_mystem = []\n",
    "    for word in ana:\n",
    "        if 'analysis' in word:\n",
    "            if word['analysis'] != []:\n",
    "                gr = word['analysis'][0]['gr']\n",
    "                pos = gr.split('=')[0].split(',')[0]\n",
    "                if pos == 'V':\n",
    "                    pos = 'VERB'\n",
    "                if pos == 'S':\n",
    "                    pos = 'NOUN'\n",
    "                if pos == 'A':\n",
    "                    pos = 'ADJ'\n",
    "                if pos == 'SPRO' or pos == 'APRO':\n",
    "                    pos = 'PRON'\n",
    "                if pos == 'PR':\n",
    "                    pos = 'ADP'\n",
    "                if pos == 'ADVPRO':\n",
    "                    pos = 'CONJ'\n",
    "            word_pos_mystem.append([word['text'], pos])\n",
    "    sent.append(word_pos_mystem)\n",
    "v_noun_pos = chunker_pos(sent, 'выразить', 'NOUN')\n",
    "verb_noun = chunker_pos(sent, 'VERB', 'NOUN')\n",
    "adj_noun = chunker_pos(sent, 'ADJ', 'NOUN')\n",
    "bigram_pos = v_noun_pos + verb_noun + adj_noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_pos_clear = []\n",
    "for bigram in bigram_pos:\n",
    "    if bigram not in bigram_pos_clear:\n",
    "        bigram_pos_clear.append(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_neg = []\n",
    "for comment in neg_coments:\n",
    "    ana = m.analyze(comment)\n",
    "    word_neg_mystem = []\n",
    "    for word in ana:\n",
    "        if 'analysis' in word:\n",
    "            if word['analysis'] != []:\n",
    "                gr = word['analysis'][0]['gr']\n",
    "                pos = gr.split('=')[0].split(',')[0]\n",
    "                if pos == 'V':\n",
    "                    pos = 'VERB'\n",
    "                if pos == 'S':\n",
    "                    pos = 'NOUN'\n",
    "                if pos == 'A':\n",
    "                    pos = 'ADJ'\n",
    "                if pos == 'SPRO' or pos == 'APRO':\n",
    "                    pos = 'PRON'\n",
    "                if pos == 'PR':\n",
    "                    pos = 'ADP'\n",
    "                if pos == 'ADVPRO':\n",
    "                    pos = 'CONJ'\n",
    "            word_neg_mystem.append([word['text'], pos])\n",
    "    sent_neg.append(word_neg_mystem)\n",
    "v_noun_neg = chunker_neg(sent_neg, 'VERB', 'что')\n",
    "verb_noun = chunker_neg(sent_neg, 'VERB', 'NOUN')\n",
    "adj_noun = chunker_neg(sent_neg, 'ADJ', 'NOUN')\n",
    "bigram_neg = v_noun_neg + verb_noun + adj_noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_neg_clear = []\n",
    "for bigram in bigram_neg:\n",
    "    if bigram not in bigram_neg_clear:\n",
    "        bigram_neg_clear.append(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_lists = {}\n",
    "freq_lists['pos'] = d_pos\n",
    "freq_lists['neg'] = d_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_lists_bg = {}\n",
    "freq_lists_bg['pos'] = bigram_pos_clear\n",
    "freq_lists_bg['neg'] = bigram_neg_clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_comment(freq_lists, text, freq_lists_bg):\n",
    "    counts = Counter()\n",
    "    for emotion, freq_list in freq_lists.items():\n",
    "        freq_list = Counter(freq_list)\n",
    "        words = [w.lower() for w in word_tokenize(text) if w.isalpha()]\n",
    "        filtered = [w for w in words if w not in sw]\n",
    "        lemmas = []\n",
    "        for word in filtered:\n",
    "            lemmas.append(morph.parse(word)[0].normal_form)\n",
    "        for word in lemmas:\n",
    "            counts[emotion] += int(freq_list[word] > 0)\n",
    "            \n",
    "    for emotion, freq_list in freq_lists.items():\n",
    "        n_gram = 2\n",
    "        count = Counter(ngrams(text.split(), n_gram)).most_common()\n",
    "        bg = dict(count).keys()\n",
    "        for bigram in bg:\n",
    "            if bigram in freq_lists_bg['pos']:\n",
    "                counts['pos'] += 1\n",
    "            if bigram in freq_lists_bg['neg']:\n",
    "                counts['neg'] += 1\n",
    "    return counts.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оформляли ипотеку на загородный дом в г. Красноярске. Хотелось бы выразить огромное спасибо сотрудникам банка- О-ву Илье и К-ву Кириллу, которые на протяжении всей сделки, начиная от заявки и до подписания договора, консультировали нас и помогли оформить ипотеку на очень выгодных условиях и по выгодной для нас программе, рассмотрев заявку индивидуально!! Побольше бы таких сотрудников, кто выполняет свою работу на 110% ))\n"
     ]
    }
   ],
   "source": [
    "test_texts = random.choice(control_group)\n",
    "print(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pos', 6), ('neg', 1)]"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_comment(freq_lists, test_texts, freq_lists_bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_detect_comment(freq_lists, test_size, freq_lists_bg):\n",
    "    results = []  # результаты\n",
    "    gold = []     # исходная эмоция\n",
    "    for emotion in ['pos', 'neg']:\n",
    "        for text in control_dict[emotion]:\n",
    "            predicted_emotion = detect_comment(freq_lists, text, freq_lists_bg)[0][0]\n",
    "            results.append(predicted_emotion)\n",
    "            gold.append(emotion)\n",
    "    print(\"RESULTS:\")\n",
    "    print(\"%d languages\" % 2)\n",
    "    print(\"Test size: %d texts per emotion\" % test_size)\n",
    "    print(\"Accuracy: %.4f\" % accuracy_score(results, gold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS:\n",
      "2 languages\n",
      "Test size: 10 texts per emotion\n",
      "Accuracy: 0.8600\n"
     ]
    }
   ],
   "source": [
    "test_detect_comment(freq_lists, 10, freq_lists_bg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy поднялась с 0.8163 (считалась в файле предыдущего дз) до 0.86. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
